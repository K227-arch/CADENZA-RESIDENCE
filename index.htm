<!DOCTYPE html>
<html lang="en">
<head>
    <title>CADENZA RESIDENCE</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="viewport" id="metaViewport" content="user-scalable=no, initial-scale=1, width=device-width, viewport-fit=cover" data-tdv-general-scale="0.5"/>
    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="default">
    <script src="lib/tdvplayer.js?v=1746115429278"></script>
    <link rel="shortcut icon" href="favicon.ico?v=1746115429278">
    <link rel="icon" sizes="48x48 32x32 16x16" href="favicon.ico?v=1746115429278">
    <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="misc/icon180.png?v=1746115429278">
    <link rel="icon" type="image/png" sizes="16x16" href="misc/icon16.png?v=1746115429278">
    <link rel="icon" type="image/png" sizes="32x32" href="misc/icon32.png?v=1746115429278">
    <link rel="icon" type="image/png" sizes="192x192" href="misc/icon192.png?v=1746115429278">
    <link rel="manifest" href="manifest.json?v=1746115429278">
    <meta name="msapplication-TileColor" content="#FFFFFF">
    <meta name="msapplication-config" content="browserconfig.xml">
    <link rel="preload" href="locale/en.txt?v=1746115429278" as="fetch" crossorigin="anonymous"/>
    <link rel="preload" href="script.js?v=1746115429278" as="script"/>
    <link rel="preload" href="media/panorama_867E0801_8A76_2071_41CC_9ED378D879DD_0/r/2/0_0.webp?v=1746115429278" as="image"/>
    <link rel="preload" href="media/panorama_867E0801_8A76_2071_41CC_9ED378D879DD_0/l/2/0_0.webp?v=1746115429278" as="image"/>
    <link rel="preload" href="media/panorama_867E0801_8A76_2071_41CC_9ED378D879DD_0/u/2/0_0.webp?v=1746115429278" as="image"/>
    <link rel="preload" href="media/panorama_867E0801_8A76_2071_41CC_9ED378D879DD_0/d/2/0_0.webp?v=1746115429278" as="image"/>
    <link rel="preload" href="media/panorama_867E0801_8A76_2071_41CC_9ED378D879DD_0/f/2/0_0.webp?v=1746115429278" as="image"/>
    <link rel="preload" href="media/panorama_867E0801_8A76_2071_41CC_9ED378D879DD_0/b/2/0_0.webp?v=1746115429278" as="image"/>
    <script src="scorm.js" type="text/javascript"></script>
    <script src="https://remote.3dvista.com/lib/tdvremote.js?v=1746115429278" type="text/javascript"></script>
    <meta name="description" content="CADENZA Residence is a modern, thoughtfully designed living space that blends elegance, comfort, and convenience. Nestled in the vibrant heart of Nakasero."/>
    <meta name="theme-color" content="#FFFFFF"/>
    <script src="script.js?v=1746115429278"></script>
    <script src="https://cdn.jsdelivr.net/npm/livekit-client/dist/livekit-client.umd.min.js"></script>
    <style type="text/css">
        html, body { height:100%; width:100%; height:100vh; width:100vw; margin:0; padding:0; overflow:hidden; }
        .fill-viewport { position:fixed; top:0; left:0; right:0; bottom:0; padding:0; margin:0; overflow: hidden; }
        .fill-viewport.landscape-left { left: env(safe-area-inset-left); }
        .fill-viewport.landscape-right { right: env(safe-area-inset-right); }
        #viewer { z-index:1; }
        #preloadContainer { z-index:2; opacity:0; background-color:rgba(255,255,255,1); transition: opacity 0.5s; -webkit-transition: opacity 0.5s; -moz-transition: opacity 0.5s; -o-transition: opacity 0.5s;}

        /* --- Custom Styles for AI Button and Status (Refractive Glass) --- */
        #aiControls {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            flex-direction: column;
            align-items: flex-end;
            gap: 10px;
            z-index: 1000; /* Ensure it's above other content */
        }

        #startAgentButton {
            /* Glassmorphism base */
            background: rgba(255, 255, 255, 0.15); /* Slightly transparent white */
            backdrop-filter: blur(10px) brightness(1.1); /* Blur and brighten background */
            -webkit-backdrop-filter: blur(10px) brightness(1.1); /* Safari support */
            border: 1px solid rgba(255, 255, 255, 0.3); /* Light border for definition */
            border-bottom-color: rgba(255, 255, 255, 0.2);
            border-right-color: rgba(255, 255, 255, 0.2);

            border-radius: 15px;
            padding: 15px 30px;
            color: white;
            font-family: 'Inter', sans-serif;
            font-size: 18px;
            font-weight: bold;
            cursor: pointer;
            box-shadow: 0 8px 32px 0 rgba(0, 0, 0, 0.37); /* Stronger, transparent shadow */
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
            outline: none;
            -webkit-tap-highlight-color: transparent;
            text-shadow: 0 1px 2px rgba(0,0,0,0.4); /* Text shadow for better readability on glass */
        }

        #startAgentButton::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: linear-gradient(
                120deg,
                rgba(255, 255, 255, 0) 30%,
                rgba(255, 255, 255, 0.4) 50%, /* More prominent white shine */
                rgba(255, 255, 255, 0) 70%
            );
            transition: transform 0.3s ease;
            transform: translateX(-100%);
        }

        #startAgentButton:hover {
            background: rgba(255, 255, 255, 0.25); /* More transparent on hover */
            box-shadow: 0 12px 40px 0 rgba(0, 0, 0, 0.45); /* Stronger shadow on hover */
            transform: translateY(-2px);
        }

        #startAgentButton:hover::before {
            transform: translateX(100%);
        }

        #startAgentButton:active {
            transform: translateY(0);
            background: rgba(255, 255, 255, 0.1); /* Less transparent on active */
            box-shadow: 0 4px 16px 0 rgba(0, 0, 0, 0.2);
        }

        /* Animation for when the user is speaking */
        @keyframes pulse {
            0% { transform: scale(1); box-shadow: 0 8px 32px 0 rgba(0, 0, 0, 0.37); }
            50% { transform: scale(1.02); box-shadow: 0 12px 40px 0 rgba(0, 0, 0, 0.45); }
            100% { transform: scale(1); box-shadow: 0 8px 32px 0 rgba(0, 0, 0, 0.37); }
        }

        .speaking-pulse {
            animation: pulse 1s infinite alternate;
        }

        #statusDisplay {
            /* Glassmorphism for status display too */
            background: rgba(0, 0, 0, 0.3); /* Darker transparent background */
            backdrop-filter: blur(8px) brightness(1.2); /* Blur and brighten */
            -webkit-backdrop-filter: blur(8px) brightness(1.2);
            border: 1px solid rgba(255, 255, 255, 0.1); /* Subtle border */
            border-bottom-color: rgba(255, 255, 255, 0.05);
            border-right-color: rgba(255, 255, 255, 0.05);

            color: white;
            padding: 10px 15px;
            border-radius: 8px;
            font-family: 'Inter', sans-serif;
            font-size: 14px;
            text-align: right;
            max-width: 250px;
            box-shadow: 0 4px 16px 0 rgba(0, 0, 0, 0.2);
            opacity: 0.9;
            transition: opacity 0.3s ease;
            text-shadow: 0 1px 1px rgba(0,0,0,0.3);
        }
        /* --- End Custom Styles --- */
    </style>
    <link rel="stylesheet" href="fonts.css?v=1746115429278">
</head>
<body>
    <div id="preloadContainer" class="fill-viewport"><div style="z-index: 4; position: absolute; overflow: hidden; display: flex; align-items: center; text-align: center; overflow: hidden; left: 0%; top: 2.62%; width: 100.00%; height: 92.72%" ><video id="video_85588E39_8A92_6091_41E0_692365203C94" width="100%" height="100%" autoplay onended="onBodyClick()" playsinline webkit-playsinline poster="media/video_85588E39_8A92_6091_41E0_692365203C94_poster_en.jpg" preload="auto"><source src="media/video_85588E39_8A92_6091_41E0_692365203C94_en.mp4" type="video/mp4">Your browser does not support the video tab</video></div><div style="z-index: 5; position: absolute; overflow: hidden; background-size: contain; background-image: url('loading/HTMLImage_8592268C_8A92_E077_41E1_2D3CA8D97D28.png'); background-repeat: no-repeat; background-position: center center; overflow: hidden; left: 0%; bottom: 77.87%; width: 17.94%; height: 22.13%" ></div><div style="z-index: 6; position: absolute; overflow: hidden; background-size: contain; background-image: url('loading/HTMLImage_9E9ABF4A_8B6F_436A_41D7_8C43B82CF5E6.png'); background-repeat: no-repeat; background-position: center center; overflow: hidden; right: 0.09%; bottom: 0.87%; width: 13.72%; height: 6.05%" ></div></div>
    <div id="viewer" class="fill-viewport"></div>

    <div id="aiControls">
        <button id="startAgentButton">
            Start AI Conversation
        </button>
        <p id="statusDisplay">Click 'Start AI Conversation'</p>
    </div>
    <audio id="agentAudio" autoplay playsinline style="display:none;"></audio> <!-- Hidden audio element -->

<script>
    // Ensure this script runs after the LivekitClient library is loaded via CDN

    document.addEventListener('DOMContentLoaded', () => {
        const startButton = document.getElementById('startAgentButton');
        const agentAudioElement = document.getElementById('agentAudio');
        const statusDisplay = document.getElementById('statusDisplay');

        let room = null; // Declare room outside to make it accessible

        // Configuration
        const TOKEN_SERVER_URL = 'http://localhost:8080/get-token'; // Your Token Server URL
        const LIVEKIT_ROOM_NAME = 'cadenza-residence-ai-chat'; // Must match your Python agent's room name
        const PARTICIPANT_NAME = '3dvista-user-' + Math.floor(Math.random() * 1000); // Unique name for this user

        // Function to fetch the token from your Token Server
        async function fetchLiveKitToken() {
            try {
                statusDisplay.textContent = "Fetching access token...";
                const response = await fetch(TOKEN_SERVER_URL, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ roomName: LIVEKIT_ROOM_NAME, participantName: PARTICIPANT_NAME })
                });
                const data = await response.json();
                if (data.token && data.url) {
                    statusDisplay.textContent = "Token fetched. Connecting to LiveKit...";
                    return { token: data.token, url: data.url };
                } else {
                    throw new Error("Invalid token data received");
                }
            } catch (error) {
                console.error('Error fetching token:', error);
                statusDisplay.textContent = `Error: ${error.message}. Is Token Server running?`;
                return null;
            }
        }

        // Function to connect to LiveKit and start audio
        async function connectToLiveKit() {
            // Disable button to prevent multiple clicks
            startButton.disabled = true;
            startButton.textContent = "Connecting...";

            const tokenData = await fetchLiveKitToken();
            if (!tokenData) {
                startButton.disabled = false;
                startButton.textContent = "Start AI Conversation";
                return;
            }

            const { token, url } = tokenData;

            // Create a new LiveKit Room instance
            room = new LivekitClient.Room();

            // Set up event listeners
            room.on(LivekitClient.RoomEvent.Connected, async () => {
                console.log('Connected to LiveKit room:', room.name);
                statusDisplay.textContent = "Connected to AI. Speaking...";
                startButton.textContent = "AI Connected";
                // For glassmorphism, we don't change background color on connected state,
                // but you could add a subtle border color change or glow if desired.
                // startButton.style.background = 'rgba(40, 167, 69, 0.2)'; // Example connected state background

                // Auto-play audio requires user gesture. This attempts it.
                try {
                    await room.startAudio(); // This needs to be triggered by user interaction
                    console.log("Audio playback started automatically.");
                } catch (error) {
                    console.warn("Autoplay blocked. User interaction needed to start audio.", error);
                    statusDisplay.textContent = "Click anywhere to enable AI's voice.";
                    // Add a one-time click listener to enable audio if blocked
                    document.body.addEventListener('click', () => {
                        if (!room.canPlaybackAudio) {
                            room.startAudio();
                            statusDisplay.textContent = "Connected. Speak to the AI.";
                        }
                    }, { once: true });
                }

                // Enable local microphone
                try {
                    await room.localParticipant.setMicrophoneEnabled(true);
                    console.log("Microphone enabled.");
                    statusDisplay.textContent = "Connected. Speak to the AI.";
                } catch (error) {
                    console.error("Failed to enable microphone:", error);
                    statusDisplay.textContent = `Error: Microphone access denied.`;
                }
            })
            .on(LivekitClient.RoomEvent.Disconnected, (reason) => {
                console.log('Disconnected from room:', reason);
                statusDisplay.textContent = `Disconnected: ${reason}.`;
                startButton.disabled = false;
                startButton.textContent = "Start AI Conversation";
                startButton.style.background = ''; // Reset background (though glass effect is transparent)
                startButton.classList.remove('speaking-pulse'); // Remove pulse if disconnected
                room = null;
            })
            .on(LivekitClient.RoomEvent.TrackSubscribed, (track, publication, participant) => {
                // Check if this is the AI agent's audio track
                if (participant.identity.includes('agent') && track.kind === LivekitClient.Track.Kind.Audio) {
                    console.log('Agent audio track subscribed:', participant.identity);
                    // Attach the audio track to your audio element
                    track.attach(agentAudioElement);
                }
            })
            .on(LivekitClient.RoomEvent.ActiveSpeakersChanged, (speakers) => {
                // Check if the local participant (user) is speaking
                const localParticipant = room.localParticipant;
                const userIsSpeaking = speakers.some(s => s.identity === localParticipant.identity);

                if (userIsSpeaking) {
                    startButton.classList.add('speaking-pulse'); // Add pulse animation
                    statusDisplay.textContent = "You are speaking...";
                } else {
                    startButton.classList.remove('speaking-pulse'); // Remove pulse animation
                    // Only update status if AI is not speaking
                    const agentSpeaking = speakers.some(s => s.identity.includes('agent'));
                    if (!agentSpeaking && room && room.connectionState === LivekitClient.ConnectionState.Connected) {
                        statusDisplay.textContent = "Connected. Speak to the AI.";
                    }
                }

                // Update status if AI is speaking
                const agentSpeaking = speakers.some(s => s.identity.includes('agent'));
                if (agentSpeaking) {
                    statusDisplay.textContent = "AI is speaking...";
                }
            })
            // Add more event listeners for data messages (e.g., transcriptions from agent)
            .on(LivekitClient.RoomEvent.DataReceived, (payload, participant) => {
                try {
                    const data = JSON.parse(new TextDecoder().decode(payload));
                    if (data.type === 'transcription' && participant.identity === room.localParticipant.identity) {
                        // Display user's own transcription feedback if your agent sends it
                        // statusDisplay.textContent = `You: ${data.text}`;
                    }
                } catch (e) {
                    console.warn("Failed to parse data message:", e);
                }
            });

            // Connect to the room
            try {
                await room.connect(url, token);
            } catch (error) {
                console.error('Failed to connect to LiveKit room:', error);
                statusDisplay.textContent = `Failed to connect: ${error.message}.`;
                startButton.disabled = false;
                startButton.textContent = "Start AI Conversation";
            }
        }

        startButton.addEventListener('click', connectToLiveKit);
    });
</script>
</body>
</html>
